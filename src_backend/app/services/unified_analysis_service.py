# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€æ·±åº¦åˆ†ææœåŠ¡ (Unified Deep Analysis Service)
ä½¿ç”¨ GPT-4o å¯¹æ–‡ç« è¿›è¡Œå…¨é¢æ·±åº¦åˆ†æï¼Œç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š
"""

from openai import OpenAI
from app.config import settings
from app.utils.sentence_splitter import split_sentences
from app.utils.error_logger import log_llm_error
import json
import json_repair
import re
import time
from typing import List, Dict, Optional


class UnifiedAnalysisService:
    """ç»Ÿä¸€æ·±åº¦åˆ†ææœåŠ¡"""

    def __init__(self):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        self.client = OpenAI(
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url
        )
        self.model = settings.default_model

    async def analyze_article(self, article_content: str, article_title: str = "") -> Dict:
        """
        æ‰§è¡Œæ–‡ç« çš„ç»Ÿä¸€æ·±åº¦åˆ†æ

        Args:
            article_content: æ–‡ç« å†…å®¹
            article_title: æ–‡ç« æ ‡é¢˜ï¼ˆå¯é€‰ï¼‰

        Returns:
            åŒ…å«æŠ¥å‘Šå’Œå…ƒæ•°æ®çš„å­—å…¸ï¼š
            {
                "report": {...},  # åˆ†ææŠ¥å‘Š JSON
                "metadata": {
                    "model": str,
                    "tokens": int,
                    "processing_time_ms": int
                }
            }
        """
        start_time = time.time()

        # 1. é¢„å¤„ç†ï¼šåˆ†å¥
        sentences = self._split_sentences(article_content)
        print(f"ğŸ“„ æ–‡ç« åˆ†å¥å®Œæˆï¼Œå…± {len(sentences)} ä¸ªå¥å­")

        # 2. æ„å»º Promptï¼ˆä¼ é€’å¥å­åˆ—è¡¨è€ŒéåŸå§‹å†…å®¹ï¼‰
        prompt = self._build_analysis_prompt(sentences, article_title)

        # 3. è°ƒç”¨ LLM
        print("ğŸ¤– å¼€å§‹è°ƒç”¨ LLM è¿›è¡Œæ·±åº¦åˆ†æ...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "ä½ æ˜¯ä¸€åä¸–ç•Œçº§çš„è·¨å­¦ç§‘ç ”ç©¶åˆ†æå¸ˆï¼Œæ‹¥æœ‰æ·±åšçš„æ‰¹åˆ¤æ€§æ€ç»´èƒ½åŠ›å’Œæ•™è‚²å¿ƒç†å­¦èƒŒæ™¯ã€‚"
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                response_format={"type": "json_object"},
                temperature=0.3,
            )
        except Exception as e:
            # è®°å½•LLMè°ƒç”¨é”™è¯¯
            log_llm_error(
                service_name="unified_analysis",
                model_name=self.model,
                error=e,
                request_data={
                    "article_title": article_title,
                    "sentence_count": len(sentences),
                    "prompt_length": len(prompt)
                }
            )
            print(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            raise

        # 4. è§£æå“åº”
        try:
            # å…ˆå°è¯•ä½¿ç”¨json_repairä¿®å¤å¯èƒ½çš„JSONæ ¼å¼é”™è¯¯
            raw_content = response.choices[0].message.content
            report_json = json_repair.repair_json(raw_content, return_objects=True, ensure_ascii=False)
            print("âœ… JSONè§£ææˆåŠŸï¼ˆä½¿ç”¨json_repairï¼‰")
        except Exception as e:
            # è®°å½•JSONè§£æé”™è¯¯
            log_llm_error(
                service_name="unified_analysis",
                model_name=self.model,
                error=e,
                request_data={
                    "response_content": response.choices[0].message.content[:500]  # åªè®°å½•å‰500å­—ç¬¦
                }
            )
            print(f"âŒ JSON è§£æå¤±è´¥: {e}")
            raise

        # 5. åå¤„ç†ï¼šæ·»åŠ  DOM è·¯å¾„
        report_json = self._add_dom_paths(report_json, sentences)

        # 6. æ·»åŠ åˆ†å¥ç»“æœåˆ°æŠ¥å‘Šï¼ˆä¾›å‰ç«¯ä½¿ç”¨ï¼‰
        report_json['sentences'] = sentences
        print(f"âœ… å·²å°† {len(sentences)} ä¸ªå¥å­æ·»åŠ åˆ°æŠ¥å‘Š")

        # 7. éªŒè¯æŠ¥å‘Š
        self._validate_report(report_json)
        print("âœ… æŠ¥å‘ŠéªŒè¯é€šè¿‡")

        # 8. è®¡ç®—å¤„ç†æ—¶é—´
        processing_time_ms = int((time.time() - start_time) * 1000)

        return {
            "report": report_json,
            "metadata": {
                "model": response.model,
                "tokens": response.usage.total_tokens,
                "processing_time_ms": processing_time_ms
            }
        }

    def _split_sentences(self, text: str) -> List[str]:
        """
        æ™ºèƒ½åˆ†å¥

        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡ç« æ‹†åˆ†æˆå¥å­åˆ—è¡¨ã€‚

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            å¥å­åˆ—è¡¨
        """
        return split_sentences(text)

    def _build_analysis_prompt(self, sentences: List[str], title: str) -> str:
        """
        æ„å»ºåˆ†æ Prompt

        Args:
            sentences: å¥å­åˆ—è¡¨ï¼ˆå·²åˆ†å¥ï¼‰
            title: æ–‡ç« æ ‡é¢˜

        Returns:
            å®Œæ•´çš„ Prompt å­—ç¬¦ä¸²
        """
        # æ ¼å¼åŒ–å¥å­åˆ—è¡¨ï¼šæ¯å¥ä¸€è¡Œï¼Œå¸¦ç¼–å·
        sentence_list = "\n".join([f"[{i}] {sentence}" for i, sentence in enumerate(sentences)])
        prompt = f"""# ä»»åŠ¡ç›®æ ‡

å¯¹ä»¥ä¸‹æ–‡ç« è¿›è¡Œå…¨é¢çš„æ·±åº¦åˆ†æï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šçš„ JSON Schema è¿”å›ç»“æœã€‚

# æ–‡ç« ä¿¡æ¯

æ ‡é¢˜ï¼š{title if title else "ï¼ˆæ— æ ‡é¢˜ï¼‰"}
å¥å­æ€»æ•°ï¼š{len(sentences)}

# æ–‡ç« å¥å­åˆ—è¡¨

ä»¥ä¸‹æ˜¯æ–‡ç« çš„å®Œæ•´å¥å­åˆ—è¡¨ï¼Œæ¯å¥éƒ½æœ‰å”¯ä¸€ç¼–å·ã€‚**è¯·ç›´æ¥ä½¿ç”¨å¥å­ç¼–å·ï¼ˆå¦‚ [5]ï¼‰æ¥å®šä½å¥å­ï¼Œä¸è¦æœç´¢å¥å­æ–‡æœ¬ã€‚**

{sentence_list}

# åˆ†æç»´åº¦

## 1. å…ƒä¿¡æ¯åˆ†æ (meta_info)

åˆ†æä»¥ä¸‹ç»´åº¦ï¼š
- **author_stance**: ä½œè€…ç«‹åœºï¼ˆå®¢è§‚/ä¸»è§‚/æ‰¹åˆ¤æ€§/è¾©æŠ¤æ€§ï¼‰
- **writing_intent**: å†™ä½œæ„å›¾ï¿½ï¿½æ•™è‚²/è¯´æœ/å¨±ä¹/è®°å½•ï¼‰
- **emotional_tone**: æƒ…æ„ŸåŸºè°ƒï¼ˆä¸­æ€§/æ¿€æ˜‚/å†·é™/è®½åˆºï¼‰
- **target_audience**: ç›®æ ‡è¯»è€…ï¼ˆä¸“ä¸šäººå£«/æ™®é€šå¤§ä¼—/å­¦ç”Ÿï¼‰
- **timeliness**: æ—¶æ•ˆæ€§ï¼ˆæ—¶æ•ˆæ€§å¼º/é•¿æœŸæœ‰æ•ˆï¼‰

## 2. æ ¸å¿ƒæ¦‚å¿µæç‚¼ (concept_sparks)

**ä¸¥æ ¼æ ‡å‡†**ï¼š
- ä»…é€‰æ‹©ç†è§£æœ¬æ–‡**æ ¸å¿ƒè®ºç‚¹**æ‰€å¿…éœ€çš„æ¦‚å¿µ
- æ¦‚å¿µå¿…é¡»æ˜¯**è¯»è€…å¯èƒ½é™Œç”Ÿ**çš„ä¸“ä¸šæœ¯è¯­æˆ–æŠ½è±¡æ¦‚å¿µ
- æ•°é‡é™åˆ¶ï¼š**3-7 ä¸ª**
- è´¨é‡ä¼˜å…ˆäºæ•°é‡ï¼šå®ç¼ºæ¯‹æ»¥

**é‡è¦**ï¼šè¯·ç›´æ¥ä½¿ç”¨ä¸Šæ–¹å¥å­åˆ—è¡¨ä¸­çš„ç¼–å·ï¼ˆå¦‚ [5]ï¼‰ï¼Œä¸è¦å°è¯•æœç´¢å¥å­æ–‡æœ¬ä½ç½®ã€‚

å¯¹äºæ¯ä¸ªæ¦‚å¿µï¼Œæä¾›ï¼š
```json
{{
  "text": "æ¦‚å¿µåŸæ–‡",
  "sentence_index": 5,  // ç›´æ¥ä½¿ç”¨å¥å­åˆ—è¡¨ä¸­çš„ç¼–å·ï¼ˆä» 0 å¼€å§‹ï¼‰
  "importance_score": 9,  // 1-10 åˆ†
  "explanation_hint": "ç”¨ä¸€ä¸ªæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ä¾‹å­è§£é‡Šè¯¥æ¦‚å¿µï¼Œå¹¶è¯´æ˜å®ƒä¸ºä½•é‡è¦ã€‚"
}}
```

## 3. è®ºè¯ç»“æ„åˆ†æ (argument_sparks)

è¯†åˆ«æ–‡ç« ä¸­çš„å…³é”®è®ºè¯å…ƒç´ ï¼š
- **æ ¸å¿ƒè§‚ç‚¹å¥** (claim): ä½œè€…çš„ä¸»è¦è®ºç‚¹
- **æ”¯æ’‘è¯æ®å¥** (evidence): æ•°æ®ã€æ¡ˆä¾‹ã€å¼•ç”¨
- **å…³é”®è½¬æŠ˜å¥** (transition): "ç„¶è€Œ"ã€"äº‹å®ä¸Š"ç­‰è½¬æŠ˜

**é‡è¦**ï¼šç›´æ¥å¼•ç”¨å¥å­åˆ—è¡¨ä¸­çš„ç¼–å·ï¼Œæ— éœ€æœç´¢æ–‡æœ¬ã€‚

å¯¹äºæ¯ä¸ªè®ºè¯ç‚¹ï¼Œæä¾›ï¼š
```json
{{
  "type": "claim",  // claim / evidence / transition
  "text": "å¥å­åŸæ–‡ï¼ˆä»å¥å­åˆ—è¡¨ä¸­å¤åˆ¶ï¼‰",
  "sentence_index": 3,  // ç›´æ¥ä½¿ç”¨å¥å­åˆ—è¡¨ä¸­çš„ç¼–å·
  "role_description": "è¿™å¥è¯åœ¨è®ºè¯ä¸­çš„ä½œç”¨"
}}
```

## 4. çŸ¥è¯†å›¾è°±èŠ‚ç‚¹ (knowledge_graph_nodes)

æå– 5-10 ä¸ªå¯ä»¥æ„å»ºçŸ¥è¯†ç½‘ç»œçš„æ ¸å¿ƒæ¦‚å¿µï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼‰ã€‚

## 5. æ–‡ç« æ‘˜è¦ä¸æ ‡ç­¾

- `summary`: 150 å­—å·¦å³çš„æ ¸å¿ƒæ‘˜è¦
- `tags`: 3-5 ä¸ªä¸»é¢˜æ ‡ç­¾

# è¾“å‡ºæ ¼å¼

ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›ç»“æœï¼š

```json
{{
  "meta_info": {{
    "author_stance": "æ‰¹åˆ¤æ€§",
    "writing_intent": "æ•™è‚²",
    "emotional_tone": "å†·é™",
    "target_audience": "æ™®é€šå¤§ä¼—",
    "timeliness": "é•¿æœŸæœ‰æ•ˆ"
  }},
  "concept_sparks": [
    {{
      "text": "å…ƒä¿¡æ¯",
      "sentence_index": 5,
      "importance_score": 9,
      "explanation_hint": "ç”¨ä¸€ä¸ªæ—¥å¸¸ç”Ÿæ´»ä¸­çš„ä¾‹å­è§£é‡Š'å…ƒä¿¡æ¯'çš„æ¦‚å¿µã€‚"
    }}
  ],
  "argument_sparks": [
    {{
      "type": "claim",
      "text": "å…ƒä¿¡æ¯æ€ç»´æ˜¯é«˜æ•ˆå­¦ä¹ çš„æ ¸å¿ƒèƒ½åŠ›ã€‚",
      "sentence_index": 3,
      "role_description": "æ–‡ç« æ ¸å¿ƒè®ºç‚¹"
    }}
  ],
  "knowledge_graph_nodes": ["å…ƒä¿¡æ¯", "æ‰¹åˆ¤æ€§æ€ç»´", "è®¤çŸ¥é—­ç¯"],
  "summary": "æœ¬æ–‡æ¢è®¨äº†...",
  "tags": ["æ€ç»´æ¨¡å‹", "å­¦ä¹ æ–¹æ³•", "æ‰¹åˆ¤æ€§æ€ç»´"]
}}
```

# è´¨é‡è¦æ±‚

1. **ç²¾å‡†æ€§**: æ‰€æœ‰å¼•ç”¨çš„æ–‡æœ¬å¿…é¡»ä¸å¥å­åˆ—è¡¨ä¸­çš„åŸæ–‡å®Œå…¨ä¸€è‡´
2. **å¯å®šä½æ€§**: sentence_index å¿…é¡»ç›´æ¥ä½¿ç”¨å¥å­åˆ—è¡¨ä¸­çš„ç¼–å·ï¼ˆä» 0 å¼€å§‹è®¡æ•°ï¼‰ï¼Œä¸è¦æœç´¢æ–‡æœ¬ä½ç½®
3. **ä»·å€¼å¯¼å‘**: åªæ ‡æ³¨çœŸæ­£æœ‰ä»·å€¼çš„å†…å®¹
4. **ç»“æ„åŒ–**: ä¸¥æ ¼éµå®ˆ JSON æ ¼å¼

**ç‰¹åˆ«æé†’**ï¼šä¸ºé¿å…å¹»è§‰ï¼Œè¯·åŠ¡å¿…ç›´æ¥ä»å¥å­åˆ—è¡¨ä¸­å¤åˆ¶å¥å­ç¼–å·ï¼Œä¸è¦å°è¯•è‡ªè¡Œè®¡ç®—æˆ–æœç´¢ä½ç½®ã€‚
"""
        return prompt

    def _add_dom_paths(self, report: Dict, sentences: List[str]) -> Dict:
        """
        ä¸ºæ¯ä¸ªç«èŠ±æ·»åŠ  DOM è·¯å¾„

        Args:
            report: åŸå§‹æŠ¥å‘Š
            sentences: å¥å­åˆ—è¡¨

        Returns:
            æ·»åŠ äº† DOM è·¯å¾„çš„æŠ¥å‘Š
        """
        # ä¸ºæ¦‚å¿µç«èŠ±æ·»åŠ  DOM è·¯å¾„
        for spark in report.get('concept_sparks', []):
            idx = spark['sentence_index']
            spark['dom_path'] = f"#sentence-{idx}"

        # ä¸ºè®ºè¯ç«èŠ±æ·»åŠ  DOM è·¯å¾„
        for spark in report.get('argument_sparks', []):
            idx = spark['sentence_index']
            spark['dom_path'] = f"#sentence-{idx}"

        return report

    def _validate_report(self, report: Dict):
        """
        éªŒè¯æŠ¥å‘Šç»“æ„

        Args:
            report: åˆ†ææŠ¥å‘Š

        Raises:
            ValueError: å¦‚æœæŠ¥å‘Šç»“æ„ä¸ç¬¦åˆè¦æ±‚
        """
        # å¿…éœ€å­—æ®µ
        required_fields = ['meta_info', 'concept_sparks', 'summary', 'tags']
        for field in required_fields:
            if field not in report:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        # éªŒè¯ meta_info
        meta_info = report['meta_info']
        required_meta_fields = ['author_stance', 'writing_intent', 'emotional_tone', 'target_audience', 'timeliness']
        for field in required_meta_fields:
            if field not in meta_info:
                raise ValueError(f"meta_info ç¼ºå°‘å­—æ®µ: {field}")

        # éªŒè¯æ¦‚å¿µç«èŠ±
        for i, spark in enumerate(report['concept_sparks']):
            required_spark_fields = ['text', 'sentence_index', 'importance_score', 'explanation_hint']
            for field in required_spark_fields:
                if field not in spark:
                    raise ValueError(f"concept_sparks[{i}] ç¼ºå°‘å­—æ®µ: {field}")

        # éªŒè¯æ ‡ç­¾æ•°é‡
        tags = report['tags']
        if not isinstance(tags, list) or len(tags) < 3 or len(tags) > 5:
            raise ValueError(f"tags æ•°é‡åº”åœ¨ 3-5 ä¸ªä¹‹é—´ï¼Œå½“å‰: {len(tags)}")

        print(f"âœ… æŠ¥å‘ŠéªŒè¯é€šè¿‡ï¼š{len(report['concept_sparks'])} ä¸ªæ¦‚å¿µç«èŠ±ï¼Œ{len(report.get('argument_sparks', []))} ä¸ªè®ºè¯ç«èŠ±")
