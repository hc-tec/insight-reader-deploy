"""æ´å¯Ÿç”Ÿæˆ API"""
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
import json
import time
import logging

from app.schemas.insight import (
    InsightRequest,
    InsightMetadata,
    ButtonGenerationRequest,
    ButtonGenerationResponse,
    FollowUpRequest,
    FollowUpButton
)
from app.services.ai_service import AIService
from app.core.task_manager import task_manager
from app.utils.auth import get_current_active_user
from app.models.models import User

logger = logging.getLogger(__name__)
router = APIRouter()


@router.post("/insights/generate")
async def generate_insight(request: InsightRequest):
    """
    æµå¼ç”Ÿæˆæ´å¯Ÿå¡ç‰‡

    Args:
        request: åŒ…å«é€‰ä¸­æ–‡æœ¬ã€ä¸Šä¸‹æ–‡å’Œæ„å›¾çš„è¯·æ±‚

    Returns:
        SSE æµå¼å“åº”
    """
    ai_service = AIService()
    request_id = f"req_{int(time.time() * 1000)}"
    start_time = time.time()

    async def event_stream():
        """SSE äº‹ä»¶æµç”Ÿæˆå™¨"""
        try:
            # 1. å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'request_id': request_id})}\n\n"

            full_content = ""
            full_reasoning = ""
            chunk_count = 0

            # 2. æµå¼ç”Ÿæˆæ´å¯Ÿ
            async for chunk in ai_service.generate_insight_stream(
                selected_text=request.selected_text,
                context=request.context,
                intent=request.intent,
                custom_question=request.custom_question,
                use_reasoning=request.use_reasoning,
                include_full_text=request.include_full_text,
                full_text=request.full_text
            ):
                # chunk å¯èƒ½åŒ…å« content æˆ– reasoning_content
                if isinstance(chunk, dict):
                    if 'reasoning' in chunk:
                        full_reasoning += chunk['reasoning']
                        # å‘é€æ¨ç†å†…å®¹
                        yield f"data: {json.dumps({'type': 'reasoning', 'content': chunk['reasoning']})}\n\n"
                    if 'content' in chunk:
                        full_content += chunk['content']
                        # å‘é€æ­£å¸¸å†…å®¹
                        yield f"data: {json.dumps({'type': 'delta', 'content': chunk['content']})}\n\n"
                else:
                    # å…¼å®¹éæ¨ç†æ¨¡å¼
                    full_content += chunk
                    chunk_count += 1
                    yield f"data: {json.dumps({'type': 'delta', 'content': chunk})}\n\n"

            # 3. è®¡ç®—å…ƒæ•°æ®
            duration_ms = int((time.time() - start_time) * 1000)

            # ç®€åŒ–çš„ token è®¡æ•°ï¼ˆå®é™…åº”è¯¥ä½¿ç”¨ tiktokenï¼‰
            # ä¸­æ–‡ï¼š1å­—â‰ˆ1.5tokensï¼Œè‹±æ–‡ï¼š1è¯â‰ˆ1.3tokens
            estimated_tokens = int((len(full_content) + len(full_reasoning)) * 1.5)

            metadata = InsightMetadata(
                model=ai_service.model_used,
                tokens=estimated_tokens,
                duration_ms=duration_ms
            )

            # 4. å‘é€å®Œæˆäº‹ä»¶
            complete_event = {
                "type": "complete",
                "full_content": full_content,
                "full_reasoning": full_reasoning if full_reasoning else None,
                "metadata": metadata.model_dump()
            }
            yield f"data: {json.dumps(complete_event)}\n\n"

        except Exception as e:
            # å‘é€é”™è¯¯äº‹ä»¶
            error_event = {
                "type": "error",
                "message": str(e),
                "code": "GENERATION_FAILED"
            }
            yield f"data: {json.dumps(error_event)}\n\n"

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # ç¦ç”¨ Nginx ç¼“å†²
        }
    )


@router.get("/insights/health")
async def health():
    """API å¥åº·æ£€æŸ¥"""
    return {"status": "ok", "endpoint": "insights"}


# ============= åå°ä»»åŠ¡å‡½æ•° =============

async def generate_buttons_task(
    selected_text: str,
    insight: str,
    intent: str,
    conversation_history: list
):
    """
    åå°æŒ‰é’®ç”Ÿæˆä»»åŠ¡

    Args:
        selected_text: é€‰ä¸­æ–‡æœ¬
        insight: æ´å¯Ÿå†…å®¹
        intent: æ„å›¾
        conversation_history: å¯¹è¯å†å²

    Returns:
        æŒ‰é’®åˆ—è¡¨
    """
    try:
        logger.info(f"[åå°ä»»åŠ¡] å¼€å§‹ç”Ÿæˆè¿½é—®æŒ‰é’®")

        ai_service = AIService()
        buttons = await ai_service.generate_follow_up_buttons(
            selected_text=selected_text,
            insight=insight,
            intent=intent,
            conversation_history=conversation_history
        )

        logger.info(f"[åå°ä»»åŠ¡] æŒ‰é’®ç”Ÿæˆå®Œæˆï¼Œå…± {len(buttons)} ä¸ª")

        return {
            "status": "completed",
            "buttons": [btn.model_dump() for btn in buttons]
        }

    except Exception as e:
        logger.error(f"[åå°ä»»åŠ¡] æŒ‰é’®ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)

        # è¿”å›é»˜è®¤æŒ‰é’®
        default_buttons = [
            {"id": "example_default", "label": "ä¸¾ä¸ªä¾‹å­", "icon": "ğŸŒ°", "category": "example"},
            {"id": "simplify_default", "label": "è¯´å¾—ç®€å•ç‚¹", "icon": "ğŸ¯", "category": "simplify"},
            {"id": "extend_default", "label": "æ·±å…¥äº†è§£", "icon": "ğŸ“š", "category": "extend"}
        ]

        return {
            "status": "completed",
            "buttons": default_buttons,
            "error": str(e)
        }


@router.post("/insights/generate-buttons")
async def generate_follow_up_buttons(
    request: ButtonGenerationRequest,
    current_user: User = Depends(get_current_active_user)
):
    """
    ç”Ÿæˆæ™ºèƒ½è¿½é—®æŒ‰é’®ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰

    Args:
        request: åŒ…å«é€‰ä¸­æ–‡æœ¬ã€æ´å¯Ÿå†…å®¹ã€æ„å›¾å’Œå¯¹è¯å†å²çš„è¯·æ±‚
        current_user: å½“å‰ç™»å½•ç”¨æˆ·ï¼ˆä» JWT è·å–ï¼‰

    Returns:
        {
            "status": "completed" | "pending",
            "buttons": [...] | null,
            "task_id": str | null
        }
    """
    # é»˜è®¤æŒ‰é’®ï¼ˆç«‹å³è¿”å›æˆ–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
    default_buttons = [
        FollowUpButton(
            id="example_default",
            label="ä¸¾ä¸ªä¾‹å­",
            icon="ğŸŒ°",
            category="example"
        ),
        FollowUpButton(
            id="simplify_default",
            label="è¯´å¾—ç®€å•ç‚¹",
            icon="ğŸ¯",
            category="simplify"
        ),
        FollowUpButton(
            id="extend_default",
            label="æ·±å…¥äº†è§£",
            icon="ğŸ“š",
            category="extend"
        )
    ]

    try:
        # æäº¤åå°ä»»åŠ¡
        task_id = task_manager.submit_task(
            "button_generation",
            generate_buttons_task,
            {
                "user_id": current_user.id,  # ä» JWT è·å–ç”¨æˆ· ID
                "insight_preview": request.insight[:50] + "..."
            },
            request.selected_text,
            request.insight,
            request.intent,
            request.conversation_history
        )

        logger.info(f"[API] æŒ‰é’®ç”Ÿæˆä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {task_id}")

        return {
            "status": "pending",
            "buttons": None,
            "task_id": task_id
        }

    except Exception as e:
        logger.error(f"[API] æäº¤æŒ‰é’®ç”Ÿæˆä»»åŠ¡å¤±è´¥: {str(e)}")

        # å¤±è´¥æ—¶ç›´æ¥è¿”å›é»˜è®¤æŒ‰é’®
        return {
            "status": "completed",
            "buttons": [btn.model_dump() for btn in default_buttons],
            "task_id": None
        }


@router.post("/insights/follow-up")
async def generate_follow_up_answer(request: FollowUpRequest):
    """
    æµå¼ç”Ÿæˆè¿½é—®å›ç­”

    Args:
        request: åŒ…å«åŸå§‹æ–‡æœ¬ã€åˆå§‹æ´å¯Ÿã€å¯¹è¯å†å²å’Œè¿½é—®é—®é¢˜çš„è¯·æ±‚

    Returns:
        SSE æµå¼å“åº”
    """
    ai_service = AIService()
    request_id = f"followup_{int(time.time() * 1000)}"
    start_time = time.time()

    async def event_stream():
        """SSE äº‹ä»¶æµç”Ÿæˆå™¨"""
        try:
            # 1. å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'request_id': request_id})}\n\n"

            full_content = ""
            full_reasoning = ""

            # 2. æµå¼ç”Ÿæˆå›ç­”
            async for chunk in ai_service.generate_follow_up_answer_stream(
                selected_text=request.selected_text,
                initial_insight=request.initial_insight,
                conversation_history=request.conversation_history,
                follow_up_question=request.follow_up_question,
                use_reasoning=request.use_reasoning
            ):
                # chunk å¯èƒ½åŒ…å« content æˆ– reasoning
                if isinstance(chunk, dict):
                    if 'reasoning' in chunk:
                        full_reasoning += chunk['reasoning']
                        yield f"data: {json.dumps({'type': 'reasoning', 'content': chunk['reasoning']})}\n\n"
                    if 'content' in chunk:
                        full_content += chunk['content']
                        yield f"data: {json.dumps({'type': 'delta', 'content': chunk['content']})}\n\n"
                else:
                    full_content += chunk
                    yield f"data: {json.dumps({'type': 'delta', 'content': chunk})}\n\n"

            # 3. è®¡ç®—å…ƒæ•°æ®
            duration_ms = int((time.time() - start_time) * 1000)
            estimated_tokens = int((len(full_content) + len(full_reasoning)) * 1.5)

            metadata = InsightMetadata(
                model=ai_service.model_used,
                tokens=estimated_tokens,
                duration_ms=duration_ms
            )

            # 4. å‘é€å®Œæˆäº‹ä»¶
            complete_event = {
                "type": "complete",
                "full_content": full_content,
                "full_reasoning": full_reasoning if full_reasoning else None,
                "metadata": metadata.model_dump()
            }
            yield f"data: {json.dumps(complete_event)}\n\n"

        except Exception as e:
            error_event = {
                "type": "error",
                "message": str(e),
                "code": "FOLLOWUP_FAILED"
            }
            yield f"data: {json.dumps(error_event)}\n\n"

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )
