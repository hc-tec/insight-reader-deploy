# -*- coding: utf-8 -*-
"""
å¥å­æ‹†åˆ†å·¥å…·

ä½¿ç”¨ stanza NLP åº“è¿›è¡Œé«˜è´¨é‡çš„ä¸­æ–‡åˆ†å¥
"""

import re
from typing import List, Dict
import stanza
from functools import lru_cache

# å…¨å±€ stanza pipeline
_nlp_pipeline = None


def _get_nlp_pipeline():
    """è·å–æˆ–åˆå§‹åŒ– stanza pipelineï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _nlp_pipeline
    if _nlp_pipeline is None:
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Stanza ä¸­æ–‡åˆ†å¥æ¨¡å‹...")
        try:
            # å°è¯•åŠ è½½ä¸­æ–‡æ¨¡å‹
            _nlp_pipeline = stanza.Pipeline(
                lang='zh',
                processors='tokenize',  # åªä½¿ç”¨åˆ†å¥åŠŸèƒ½ï¼Œé€Ÿåº¦æ›´å¿«
                tokenize_no_ssplit=False,  # å¯ç”¨å¥å­æ‹†åˆ†
                use_gpu=False,  # æ ¹æ®ä½ çš„ç¯å¢ƒå¯ä»¥è®¾ç½®ä¸º True
                download_method=None  # ä¸è‡ªåŠ¨ä¸‹è½½ï¼Œå‡è®¾å·²å®‰è£…
            )
            print("âœ… Stanza ä¸­æ–‡åˆ†å¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Stanza æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·è¿è¡Œ: python -m stanza.download('zh')")
            raise RuntimeError(
                "Stanza ä¸­æ–‡æ¨¡å‹æœªå®‰è£…ã€‚è¯·è¿è¡Œ: python -m stanza.download('zh')"
            ) from e
    return _nlp_pipeline


class SentenceSplitter:
    """å¥å­æ‹†åˆ†å™¨ - ä½¿ç”¨ Stanza NLP è¿›è¡Œé«˜è´¨é‡ä¸­æ–‡åˆ†å¥"""

    @staticmethod
    def split_into_sentences(text: str, use_stanza: bool = True) -> List[str]:
        """
        å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­åˆ—è¡¨

        Args:
            text: çº¯æ–‡æœ¬ï¼ˆä¸åŒ…å« HTML æ ‡ç­¾ï¼‰
            use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼‰
                       False åˆ™ä½¿ç”¨ç®€å•æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ›´å¿«ä½†ç²¾åº¦ä½ï¼‰

        Returns:
            å¥å­æ•°ç»„
        """
        if not text or not text.strip():
            return []

        # é¢„å¤„ç†ï¼šç»Ÿä¸€æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        if use_stanza:
            return SentenceSplitter._split_with_stanza(text)
        else:
            return SentenceSplitter._split_with_regex(text)

    @staticmethod
    def _split_with_stanza(text: str) -> List[str]:
        """
        ä½¿ç”¨ Stanza NLP è¿›è¡Œåˆ†å¥ï¼ˆæ¨èï¼‰

        ä¼˜ç‚¹ï¼š
        - å‡†ç¡®è¯†åˆ«ä¸­æ–‡å¥å­è¾¹ç•Œ
        - æ­£ç¡®å¤„ç†å¼•å·ã€æ‹¬å·å†…çš„å¥å­
        - å¤„ç†çœç•¥å·ã€æ„Ÿå¹å·ã€é—®å·çš„å¤æ‚ç»„åˆ
        - é¿å…è¯¯æ‹†åˆ†ï¼ˆå¦‚äººåä¸­çš„ç‚¹ã€ç¼©å†™ç­‰ï¼‰
        """
        try:
            nlp = _get_nlp_pipeline()
            doc = nlp(text)

            sentences = []
            for sentence in doc.sentences:
                sentence_text = sentence.text.strip()
                if sentence_text:
                    sentences.append(sentence_text)

            return sentences

        except Exception as e:
            print(f"âš ï¸ Stanza åˆ†å¥å¤±è´¥ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•: {e}")
            return SentenceSplitter._split_with_regex(text)

    @staticmethod
    def _split_with_regex(text: str) -> List[str]:
        """
        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œåˆ†å¥ï¼ˆå¿«é€Ÿä½†ç²¾åº¦è¾ƒä½ï¼‰

        è§„åˆ™ï¼š
        - ä¸­æ–‡ï¼šæŒ‰ ã€‚ï¼ï¼Ÿâ€¦\n æ‹†åˆ†
        - è‹±æ–‡ï¼šæŒ‰ .!?\n æ‹†åˆ†ï¼Œå¤„ç†å¸¸è§ç¼©å†™
        - ä¿ç•™æ ‡ç‚¹ç¬¦å·
        - è¿‡æ»¤ç©ºå¥å­
        """
        # å¸¸è§è‹±æ–‡ç¼©å†™
        ABBREVIATIONS = [
            'Mr.', 'Mrs.', 'Ms.', 'Dr.', 'Prof.', 'Sr.', 'Jr.',
            'etc.', 'vs.', 'e.g.', 'i.e.', 'U.S.', 'U.K.',
            'Inc.', 'Ltd.', 'Co.', 'Corp.'
        ]

        # å¥å­åˆ†éš”ç¬¦ï¼ˆä¸­æ–‡å’Œè‹±æ–‡ï¼‰
        sentence_delimiter_pattern = r'([ã€‚ï¼ï¼Ÿ!?.â€¦]+[\s\n]|[\n])'

        # æ‹†åˆ†å¹¶ä¿ç•™åˆ†éš”ç¬¦
        parts = re.split(sentence_delimiter_pattern, text)

        sentences = []
        current_sentence = ''

        for i in range(len(parts)):
            part = parts[i]

            if not part:
                continue

            # å¦‚æœæ˜¯åˆ†éš”ç¬¦
            if re.match(sentence_delimiter_pattern, part):
                current_sentence += part

                # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„å¥å­ç»“æŸï¼ˆæ’é™¤ç¼©å†™ç­‰ï¼‰
                next_part = parts[i + 1] if i + 1 < len(parts) else None
                if _is_valid_sentence_end(current_sentence, next_part, ABBREVIATIONS):
                    trimmed = current_sentence.strip()
                    if trimmed:
                        sentences.append(trimmed)
                        current_sentence = ''
            else:
                current_sentence += part

        # å¤„ç†æœ€åä¸€ä¸ªå¥å­
        trimmed = current_sentence.strip()
        if trimmed:
            sentences.append(trimmed)

        return [s for s in sentences if s]

    @staticmethod
    def split_into_paragraphs(text: str, use_stanza: bool = True) -> List[Dict]:
        """
        å°†çº¯æ–‡æœ¬æ‹†åˆ†ä¸ºæ®µè½å’Œå¥å­

        Args:
            text: çº¯æ–‡æœ¬
            use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            æ®µè½æ•°ç»„ï¼Œæ¯ä¸ªæ®µè½åŒ…å«å¥å­åˆ—è¡¨
            [
                {
                    "index": 0,
                    "sentences": [
                        {"index": 0, "text": "...", "paragraph_index": 0},
                        {"index": 1, "text": "...", "paragraph_index": 0}
                    ]
                }
            ]
        """
        # æŒ‰åŒæ¢è¡Œæ‹†åˆ†æ®µè½
        raw_paragraphs = [p.strip() for p in re.split(r'\n\n+', text) if p.strip()]

        paragraphs = []
        global_sentence_index = 0

        for paragraph_index, paragraph_text in enumerate(raw_paragraphs):
            # æ‹†åˆ†è¯¥æ®µè½çš„å¥å­
            sentence_texts = SentenceSplitter.split_into_sentences(
                paragraph_text,
                use_stanza=use_stanza
            )

            sentences = []
            for sentence_text in sentence_texts:
                sentences.append({
                    "index": global_sentence_index,
                    "text": sentence_text,
                    "paragraph_index": paragraph_index
                })
                global_sentence_index += 1

            paragraphs.append({
                "index": paragraph_index,
                "sentences": sentences
            })

        return paragraphs


def _is_valid_sentence_end(sentence: str, next_part: str = None, abbreviations: List[str] = None) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¥å­ç»“æŸ

    æ’é™¤å¸¸è§ç¼©å†™ï¼š
    - Mr. Mrs. Dr. Prof. etc.
    - U.S. U.K. etc.
    """
    if abbreviations is None:
        abbreviations = []

    # å¦‚æœå¥å­ä»¥æ¢è¡Œç»“æŸï¼Œä¸€å®šæ˜¯å¥å­ç»“æŸ
    if sentence.endswith('\n'):
        return True

    # æ£€æŸ¥æ˜¯å¦ä»¥ç¼©å†™ç»“å°¾
    for abbr in abbreviations:
        if sentence.strip().endswith(abbr):
            # å¦‚æœä¸‹ä¸€ä¸ªéƒ¨åˆ†ä»¥å°å†™å­—æ¯å¼€å¤´ï¼Œè¯´æ˜ä¸æ˜¯å¥å­ç»“æŸ
            if next_part and re.match(r'^[a-z]', next_part.strip()):
                return False

    return True


# ä¾¿æ·å‡½æ•°
def split_sentences(text: str, use_stanza: bool = True) -> List[str]:
    """
    æ‹†åˆ†å¥å­

    Args:
        text: è¾“å…¥æ–‡æœ¬
        use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼Œæ¨èï¼‰

    Returns:
        å¥å­åˆ—è¡¨
    """
    return SentenceSplitter.split_into_sentences(text, use_stanza=use_stanza)


def split_paragraphs(text: str, use_stanza: bool = True) -> List[Dict]:
    """
    æ‹†åˆ†æ®µè½å’Œå¥å­

    Args:
        text: è¾“å…¥æ–‡æœ¬
        use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼Œæ¨èï¼‰

    Returns:
        æ®µè½å’Œå¥å­çš„ç»“æ„åŒ–æ•°æ®
    """
    return SentenceSplitter.split_into_paragraphs(text, use_stanza=use_stanza)
