# -*- coding: utf-8 -*-
"""
å¥å­æ‹†åˆ†å·¥å…·

ä½¿ç”¨ stanza NLP åº“è¿›è¡Œé«˜è´¨é‡çš„ä¸­æ–‡åˆ†å¥
"""

import re
from typing import List, Dict
import stanza
from functools import lru_cache

# å…¨å±€ stanza pipeline
_nlp_pipeline = None


def _get_nlp_pipeline():
    """è·å–æˆ–åˆå§‹åŒ– stanza pipelineï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _nlp_pipeline
    if _nlp_pipeline is None:
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– Stanza ä¸­æ–‡åˆ†å¥æ¨¡å‹...")
        try:
            # å°è¯•åŠ è½½ä¸­æ–‡æ¨¡å‹
            _nlp_pipeline = stanza.Pipeline(
                lang='zh',
                processors='tokenize',  # åªä½¿ç”¨åˆ†å¥åŠŸèƒ½ï¼Œé€Ÿåº¦æ›´å¿«
                tokenize_no_ssplit=False,  # å¯ç”¨å¥å­æ‹†åˆ†
                use_gpu=False,  # æ ¹æ®ä½ çš„ç¯å¢ƒå¯ä»¥è®¾ç½®ä¸º True
                download_method=None  # ä¸è‡ªåŠ¨ä¸‹è½½ï¼Œå‡è®¾å·²å®‰è£…
            )
            print("âœ… Stanza ä¸­æ–‡åˆ†å¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Stanza æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·è¿è¡Œ: python -m stanza.download('zh')")
            raise RuntimeError(
                "Stanza ä¸­æ–‡æ¨¡å‹æœªå®‰è£…ã€‚è¯·è¿è¡Œ: python -m stanza.download('zh')"
            ) from e
    return _nlp_pipeline


class SentenceSplitter:
    """å¥å­æ‹†åˆ†å™¨ - ä½¿ç”¨ Stanza NLP è¿›è¡Œé«˜è´¨é‡ä¸­æ–‡åˆ†å¥"""

    @staticmethod
    def split_into_sentences(text: str, use_stanza: bool = True) -> List[str]:
        """
        å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­åˆ—è¡¨

        Args:
            text: çº¯æ–‡æœ¬ï¼ˆä¸åŒ…å« HTML æ ‡ç­¾ï¼‰
            use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼‰
                       False åˆ™ä½¿ç”¨ç®€å•æ­£åˆ™è¡¨è¾¾å¼ï¼ˆæ›´å¿«ä½†ç²¾åº¦ä½ï¼‰

        Returns:
            å¥å­æ•°ç»„
        """
        if not text or not text.strip():
            return []

        # é¢„å¤„ç†ï¼šç»Ÿä¸€æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        if use_stanza:
            return SentenceSplitter._split_with_stanza(text)
        else:
            return SentenceSplitter._split_with_regex(text)

    @staticmethod
    def _split_with_stanza(text: str) -> List[str]:
        """
        ä½¿ç”¨ Stanza NLP è¿›è¡Œåˆ†å¥ï¼ˆæ¨èï¼‰

        ä¼˜ç‚¹ï¼š
        - å‡†ç¡®è¯†åˆ«ä¸­æ–‡å¥å­è¾¹ç•Œ
        - æ­£ç¡®å¤„ç†å¼•å·ã€æ‹¬å·å†…çš„å¥å­
        - å¤„ç†çœç•¥å·ã€æ„Ÿå¹å·ã€é—®å·çš„å¤æ‚ç»„åˆ
        - é¿å…è¯¯æ‹†åˆ†ï¼ˆå¦‚äººåä¸­çš„ç‚¹ã€ç¼©å†™ç­‰ï¼‰
        """
        try:
            nlp = _get_nlp_pipeline()
            doc = nlp(text)

            sentences = []
            for sentence in doc.sentences:
                sentence_text = sentence.text.strip()
                if sentence_text:
                    sentences.append(sentence_text)

            return sentences

        except Exception as e:
            print(f"âš ï¸ Stanza åˆ†å¥å¤±è´¥ï¼Œå›é€€åˆ°æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•: {e}")
            return SentenceSplitter._split_with_regex(text)

    @staticmethod
    def _split_with_enhanced_rules(text: str) -> List[str]:
        """
        ä½¿ç”¨å¢å¼ºè§„åˆ™è¿›è¡Œåˆ†å¥

        è§„åˆ™ä¼˜åŒ–ï¼š
        1. è¯†åˆ«ä¸­æ–‡å¥å­ç»“æŸæ ‡ç‚¹ï¼šã€‚ï¼ï¼Ÿï¼›â€¦
        2. è¯†åˆ«è‹±æ–‡å¥å­ç»“æŸæ ‡ç‚¹ï¼š.!?;
        3. å¤„ç†å¼•å·ã€æ‹¬å·å†…çš„å¥å­
        4. å¤„ç†çœç•¥å·ã€æ„Ÿå¹å·çš„ç»„åˆ
        5. å¤„ç†è‹±æ–‡ç¼©å†™ï¼ˆé¿å…è¯¯æ‹†åˆ†ï¼‰
        6. å¤„ç†æ¢è¡Œç¬¦ï¼ˆå¼ºåˆ¶å¥å­è¾¹ç•Œï¼‰
        7. å¤„ç†æ•°å­—ã€æ—¥æœŸï¼ˆé¿å…è¯¯æ‹†åˆ†ï¼‰
        """
        sentences = []

        # æ­¥éª¤1: æŒ‰æ®µè½é¢„å¤„ç†
        paragraphs = text.split('\n\n')

        for para in paragraphs:
            if not para.strip():
                continue

            # æ­¥éª¤2: åœ¨æ®µè½å†…è¿›è¡Œåˆ†å¥
            para_sentences = SentenceSplitter._split_paragraph(para)
            sentences.extend(para_sentences)

        return [s.strip() for s in sentences if s.strip()]

    @staticmethod
    def _split_paragraph(text: str) -> List[str]:
        """åœ¨æ®µè½å†…è¿›è¡Œç²¾ç»†åˆ†å¥"""

        # å¥å­åˆ†éš”ç¬¦çš„æ­£åˆ™æ¨¡å¼
        # åŒ¹é…ï¼šä¸­æ–‡æ ‡ç‚¹(ã€‚ï¼ï¼Ÿï¼›â€¦) + å¯é€‰ç©ºç™½
        #      è‹±æ–‡æ ‡ç‚¹(.!?;) + ç©ºç™½
        #      æ¢è¡Œç¬¦
        pattern = r'([ã€‚ï¼ï¼Ÿï¼›â€¦]+\s*|[.!?]+\s+|\n)'

        parts = re.split(pattern, text)

        sentences = []
        current = ''
        i = 0

        while i < len(parts):
            part = parts[i]

            if not part:
                i += 1
                continue

            # å¦‚æœæ˜¯åˆ†éš”ç¬¦
            if re.match(pattern, part):
                current += part

                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœ¨æ­¤å¤„æ–­å¥
                next_part = parts[i + 1] if i + 1 < len(parts) else ''

                if SentenceSplitter._should_break(current, next_part):
                    sentences.append(current.strip())
                    current = ''
                # å¦åˆ™ç»§ç»­ç´¯ç§¯
            else:
                current += part

            i += 1

        # æ·»åŠ æœ€åä¸€ä¸ªå¥å­
        if current.strip():
            sentences.append(current.strip())

        return sentences

    @staticmethod
    def _should_break(current: str, next_part: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨å½“å‰ä½ç½®æ–­å¥

        Args:
            current: å½“å‰ç´¯ç§¯çš„æ–‡æœ¬
            next_part: ä¸‹ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µ

        Returns:
            æ˜¯å¦åº”è¯¥æ–­å¥
        """
        current = current.strip()

        # ç©ºæ–‡æœ¬ï¼Œä¸æ–­å¥
        if not current:
            return False

        # ä»¥æ¢è¡Œç¬¦ç»“æŸï¼Œå¼ºåˆ¶æ–­å¥
        if current.endswith('\n'):
            return True

        # æ£€æŸ¥æ˜¯å¦ä»¥è‹±æ–‡ç¼©å†™ç»“å°¾
        for abbr in SentenceSplitter.ABBREVIATIONS:
            if current.endswith(abbr):
                # å¦‚æœä¸‹ä¸€éƒ¨åˆ†ä»¥å°å†™å­—æ¯å¼€å¤´ï¼Œä¸æ–­å¥
                if next_part and re.match(r'^[a-z]', next_part.strip()):
                    return False

        # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­— + ç‚¹ï¼ˆå¦‚ï¼š3.14, 1.5ï¼‰
        if re.search(r'\d+\.\s*$', current):
            if next_part and re.match(r'^\d', next_part.strip()):
                return False

        # æ£€æŸ¥æ˜¯å¦åœ¨å¼•å·æˆ–æ‹¬å·å†…
        # å¦‚æœå¼•å·ã€æ‹¬å·æœªé—­åˆï¼Œä¸æ–­å¥
        open_quotes = current.count('"') + current.count('"') + current.count('ã€Œ')
        close_quotes = current.count('"') + current.count('"') + current.count('ã€')
        open_parens = current.count('(') + current.count('ï¼ˆ')
        close_parens = current.count(')') + current.count('ï¼‰')

        if open_quotes % 2 != 0 or open_quotes != close_quotes:
            return False
        if open_parens != close_parens:
            return False

        # ä»¥ä¸­æ–‡å¥å­ç»“æŸæ ‡ç‚¹ç»“å°¾ï¼Œæ–­å¥
        if re.search(r'[ã€‚ï¼ï¼Ÿï¼›â€¦]$', current):
            return True

        # ä»¥è‹±æ–‡å¥å­ç»“æŸæ ‡ç‚¹ + ç©ºç™½ç»“å°¾ï¼Œæ–­å¥
        if re.search(r'[.!?]\s+$', current):
            return True

        # é»˜è®¤ä¸æ–­å¥
        return False

    @staticmethod
    def split_into_paragraphs(text: str, use_stanza: bool = True) -> List[Dict]:
        """
        å°†çº¯æ–‡æœ¬æ‹†åˆ†ä¸ºæ®µè½å’Œå¥å­

        Args:
            text: çº¯æ–‡æœ¬
            use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼‰

        Returns:
            æ®µè½æ•°ç»„ï¼Œæ¯ä¸ªæ®µè½åŒ…å«å¥å­åˆ—è¡¨
            [
                {
                    "index": 0,
                    "sentences": [
                        {"index": 0, "text": "...", "paragraph_index": 0},
                        {"index": 1, "text": "...", "paragraph_index": 0}
                    ]
                }
            ]
        """
        # æŒ‰åŒæ¢è¡Œæ‹†åˆ†æ®µè½
        raw_paragraphs = [p.strip() for p in re.split(r'\n\n+', text) if p.strip()]

        paragraphs = []
        global_sentence_index = 0

        for paragraph_index, paragraph_text in enumerate(raw_paragraphs):
            # æ‹†åˆ†è¯¥æ®µè½çš„å¥å­
            sentence_texts = SentenceSplitter.split_into_sentences(
                paragraph_text,
                use_stanza=use_stanza
            )

            sentences = []
            for sentence_text in sentence_texts:
                sentences.append({
                    "index": global_sentence_index,
                    "text": sentence_text,
                    "paragraph_index": paragraph_index
                })
                global_sentence_index += 1

            paragraphs.append({
                "index": paragraph_index,
                "sentences": sentences
            })

        return paragraphs


def _is_valid_sentence_end(sentence: str, next_part: str = None, abbreviations: List[str] = None) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¥å­ç»“æŸ

    æ’é™¤å¸¸è§ç¼©å†™ï¼š
    - Mr. Mrs. Dr. Prof. etc.
    - U.S. U.K. etc.
    """
    if abbreviations is None:
        abbreviations = []

    # å¦‚æœå¥å­ä»¥æ¢è¡Œç»“æŸï¼Œä¸€å®šæ˜¯å¥å­ç»“æŸ
    if sentence.endswith('\n'):
        return True

    # æ£€æŸ¥æ˜¯å¦ä»¥ç¼©å†™ç»“å°¾
    for abbr in abbreviations:
        if sentence.strip().endswith(abbr):
            # å¦‚æœä¸‹ä¸€ä¸ªéƒ¨åˆ†ä»¥å°å†™å­—æ¯å¼€å¤´ï¼Œè¯´æ˜ä¸æ˜¯å¥å­ç»“æŸ
            if next_part and re.match(r'^[a-z]', next_part.strip()):
                return False

    return True


# ä¾¿æ·å‡½æ•°
def split_sentences(text: str, use_stanza: bool = True) -> List[str]:
    """
    æ‹†åˆ†å¥å­

    Args:
        text: è¾“å…¥æ–‡æœ¬
        use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼Œæ¨èï¼‰

    Returns:
        å¥å­åˆ—è¡¨
    """
    return SentenceSplitter.split_into_sentences(text, use_stanza=use_stanza)


def split_paragraphs(text: str, use_stanza: bool = True) -> List[Dict]:
    """
    æ‹†åˆ†æ®µè½å’Œå¥å­

    Args:
        text: è¾“å…¥æ–‡æœ¬
        use_stanza: æ˜¯å¦ä½¿ç”¨ stanza NLPï¼ˆé»˜è®¤ Trueï¼Œæ¨èï¼‰

    Returns:
        æ®µè½å’Œå¥å­çš„ç»“æ„åŒ–æ•°æ®
    """
    return SentenceSplitter.split_into_paragraphs(text, use_stanza=use_stanza)
