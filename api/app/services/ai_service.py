"""AI æœåŠ¡ - OpenAI API é›†æˆ"""
from openai import AsyncOpenAI
from app.config import settings
from app.utils.prompt_templates import PromptTemplates
from app.schemas.insight import FollowUpButton, Message
from app.utils.error_logger import log_llm_error
import json
import logging

logger = logging.getLogger(__name__)


class AIService:
    """AI æœåŠ¡ç±»"""

    def __init__(self):
        """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯"""
        self.client = AsyncOpenAI(
            api_key=settings.openai_api_key,
            base_url=settings.openai_base_url
        )
        self.model_used = ""

    def select_model(self, intent: str, text_length: int, use_reasoning: bool = False) -> str:
        """
        æ ¹æ®æ„å›¾å’Œæ–‡æœ¬é•¿åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹

        Args:
            intent: ç”¨æˆ·æ„å›¾
            text_length: é€‰ä¸­æ–‡æœ¬é•¿åº¦
            use_reasoning: æ˜¯å¦ä½¿ç”¨æ¨ç†æ¨¡å‹

        Returns:
            æ¨¡å‹åç§°
        """
        # å¦‚æœå¯ç”¨æ¨ç†æ¨¡å¼ï¼Œä½¿ç”¨æ”¯æŒæ¨ç†çš„æ¨¡å‹
        if use_reasoning:
            return settings.reasoning_model  # ä¾‹å¦‚ deepseek-reasoner

        # ç®€å•è§£é‡Š + çŸ­æ–‡æœ¬ â†’ gpt-4o-mini (ä¾¿å®œå¿«é€Ÿ)
        if intent == "explain" and text_length < 10:
            return settings.simple_model

        # é»˜è®¤ä½¿ç”¨ gpt-4o (è´¨é‡ä¼˜å…ˆ)
        return settings.default_model

    async def generate_insight_stream(
        self,
        selected_text: str,
        context: str,
        intent: str,
        custom_question: str | None = None,
        use_reasoning: bool = False,
        custom_prompt: str | None = None,
        include_full_text: bool = False,
        full_text: str | None = None
    ):
        """
        æµå¼ç”Ÿæˆæ´å¯Ÿ

        Args:
            selected_text: ç”¨æˆ·é€‰ä¸­çš„æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡
            intent: ç”¨æˆ·æ„å›¾
            custom_question: è‡ªå®šä¹‰é—®é¢˜(å¯é€‰)
            use_reasoning: æ˜¯å¦ä½¿ç”¨æ¨ç†æ¨¡å‹
            custom_prompt: è‡ªå®šä¹‰ prompt (ç”¨äºç«èŠ±æ´å¯Ÿç­‰ç‰¹æ®Šåœºæ™¯)
            include_full_text: æ˜¯å¦é™„å¸¦å…¨æ–‡
            full_text: å®Œæ•´æ–‡ç« å†…å®¹(å¯é€‰)

        Yields:
            str | dict: AI ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µæˆ–åŒ…å«æ¨ç†å†…å®¹çš„å­—å…¸
        """
        # é€‰æ‹©æ¨¡å‹
        self.model_used = self.select_model(intent, len(selected_text), use_reasoning)

        # æ„å»º Prompt
        if custom_prompt:
            # ä½¿ç”¨è‡ªå®šä¹‰ prompt (ç«èŠ±æ´å¯Ÿåœºæ™¯)
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é˜…è¯»è¾…åŠ© AIï¼Œå¸®åŠ©ç”¨æˆ·æ·±å…¥ç†è§£æ–‡ç« å†…å®¹ã€‚"
            user_prompt = custom_prompt
        else:
            # ä½¿ç”¨æ ‡å‡† prompt æ¨¡æ¿
            system_prompt = PromptTemplates.get_system_prompt(intent)
            user_prompt = PromptTemplates.get_user_prompt(
                intent=intent,
                selected_text=selected_text,
                context=context,
                custom_question=custom_question,
                include_full_text=include_full_text,
                full_text=full_text
            )

        # è°ƒç”¨ OpenAI API (æµå¼)
        try:
            stream = await self.client.chat.completions.create(
                model=self.model_used,
                max_tokens=settings.max_tokens,
                temperature=settings.temperature,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                stream=True
            )

            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta

                    # æ”¯æŒæ¨ç†æ¨¡å¼
                    if use_reasoning:
                        result = {}
                        # DeepSeek æ¨ç†æ¨¡å‹ä¼šæœ‰ reasoning_content å­—æ®µ
                        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                            result['reasoning'] = delta.reasoning_content
                        if delta.content:
                            result['content'] = delta.content

                        if result:  # åªåœ¨æœ‰å†…å®¹æ—¶è¿”å›
                            yield result
                    else:
                        # éæ¨ç†æ¨¡å¼ï¼Œç›´æ¥è¿”å›å†…å®¹
                        if delta.content:
                            yield delta.content
        except Exception as e:
            # è®°å½•LLMè°ƒç”¨é”™è¯¯
            log_llm_error(
                service_name="ai_service_generate_insight",
                model_name=self.model_used,
                error=e,
                request_data={
                    "intent": intent,
                    "selected_text_length": len(selected_text),
                    "use_reasoning": use_reasoning
                }
            )
            logger.error(f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}")
            raise

    async def generate_follow_up_buttons(
        self,
        selected_text: str,
        insight: str,
        intent: str,
        conversation_history: list[Message]
    ) -> list[FollowUpButton]:
        """
        ç”Ÿæˆæ™ºèƒ½è¿½é—®æŒ‰é’®

        Args:
            selected_text: åŸå§‹é€‰ä¸­æ–‡æœ¬
            insight: å½“å‰æ´å¯Ÿå†…å®¹
            intent: åŸå§‹æ„å›¾
            conversation_history: å¯¹è¯å†å²

        Returns:
            è¿½é—®æŒ‰é’®åˆ—è¡¨ï¼ˆ3-4ä¸ªï¼‰
        """
        # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
        history_str = ""
        if conversation_history:
            for msg in conversation_history[-5:]:  # åªå–æœ€è¿‘5è½®
                role_label = "ç”¨æˆ·" if msg.role == "user" else "AI"
                history_str += f"{role_label}: {msg.content}\n"

        # æ„å»º prompt
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹ã€‚æ ¹æ®ç”¨æˆ·å½“å‰çš„å­¦ä¹ å†…å®¹å’Œå¯¹è¯å†å²ï¼Œç”Ÿæˆ3-4ä¸ªæœ€æœ‰ä»·å€¼çš„è¿½é—®å»ºè®®ã€‚

è¿½é—®åˆ†ç±»ï¼š
- example: ä¸¾ä¾‹è¯´æ˜ï¼Œå¸®åŠ©ç†è§£æŠ½è±¡æ¦‚å¿µ
- simplify: ç”¨æ›´ç®€å•çš„æ–¹å¼è§£é‡Š
- compare: å¯¹æ¯”ç›¸å…³æ¦‚å¿µï¼Œç†è§£åŒºåˆ«
- extend: å»¶ä¼¸é˜…è¯»ï¼Œæ·±å…¥å­¦ä¹ 

è¦æ±‚ï¼š
1. è¿½é—®è¦å…·ä½“ã€æœ‰ä»·å€¼ï¼Œä¸èƒ½å¤ªæ³›æ³›
2. é¿å…é‡å¤ä¹‹å‰å·²ç»è¿½é—®è¿‡çš„å†…å®¹
3. é—®é¢˜è¦ç®€çŸ­ï¼ˆ5-15å­—ï¼‰
4. è¿”å›çº¯ JSON æ ¼å¼ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—"""

        user_prompt = f"""åŸå§‹é€‰ä¸­æ–‡æœ¬: {selected_text}

å½“å‰æ´å¯Ÿ: {insight}

åŸå§‹æ„å›¾: {intent}

å¯¹è¯å†å²:
{history_str if history_str else "ï¼ˆæ— ï¼‰"}

è¯·ç”Ÿæˆ3-4ä¸ªè¿½é—®å»ºè®®ï¼ŒJSONæ ¼å¼å¦‚ä¸‹ï¼š
[
  {{"id": "example_1", "label": "ä¸¾ä¸ªå®é™…ä¾‹å­", "icon": "ğŸŒ°", "category": "example"}},
  {{"id": "simplify_1", "label": "è¯´å¾—æ›´ç®€å•ç‚¹", "icon": "ğŸ¯", "category": "simplify"}},
  {{"id": "compare_1", "label": "å’ŒXXçš„åŒºåˆ«ï¼Ÿ", "icon": "ğŸ”", "category": "compare"}}
]"""

        try:
            response = await self.client.chat.completions.create(
                model=settings.simple_model,  # ä½¿ç”¨å¿«é€Ÿæ¨¡å‹ç”ŸæˆæŒ‰é’®
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.7,
                max_tokens=500
            )

            content = response.choices[0].message.content.strip()

            # è§£æ JSON
            try:
                buttons_data = json.loads(content)
            except json.JSONDecodeError as e:
                # è®°å½•JSONè§£æé”™è¯¯
                log_llm_error(
                    service_name="ai_service_follow_up_buttons",
                    model_name=settings.simple_model,
                    error=e,
                    request_data={
                        "response_content": content[:500]
                    }
                )
                logger.error(f"âŒ JSON è§£æå¤±è´¥: {e}")
                raise

            # è½¬æ¢ä¸º FollowUpButton å¯¹è±¡
            buttons = [FollowUpButton(**btn) for btn in buttons_data]

            return buttons[:4]  # æœ€å¤šè¿”å›4ä¸ª

        except json.JSONDecodeError:
            # JSONè§£æé”™è¯¯å·²ç»åœ¨å†…éƒ¨try-catchå¤„ç†ï¼Œè¿™é‡Œç›´æ¥è¿”å›é»˜è®¤æŒ‰é’®
            logger.warning("JSONè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤æŒ‰é’®")
            return [
                FollowUpButton(
                    id="example_default",
                    label="ä¸¾ä¸ªä¾‹å­",
                    icon="ğŸŒ°",
                    category="example"
                ),
                FollowUpButton(
                    id="simplify_default",
                    label="è¯´å¾—ç®€å•ç‚¹",
                    icon="ğŸ¯",
                    category="simplify"
                )
            ]
        except Exception as e:
            # è®°å½•LLMè°ƒç”¨é”™è¯¯
            log_llm_error(
                service_name="ai_service_follow_up_buttons",
                model_name=settings.simple_model,
                error=e,
                request_data={
                    "selected_text_length": len(selected_text),
                    "insight_length": len(insight),
                    "intent": intent
                }
            )
            logger.error(f"âŒ ç”Ÿæˆè¿½é—®æŒ‰é’®å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤æŒ‰é’®
            return [
                FollowUpButton(
                    id="example_default",
                    label="ä¸¾ä¸ªä¾‹å­",
                    icon="ğŸŒ°",
                    category="example"
                ),
                FollowUpButton(
                    id="simplify_default",
                    label="è¯´å¾—ç®€å•ç‚¹",
                    icon="ğŸ¯",
                    category="simplify"
                )
            ]

    async def generate_follow_up_answer_stream(
        self,
        selected_text: str,
        initial_insight: str,
        conversation_history: list[Message],
        follow_up_question: str,
        use_reasoning: bool = False
    ):
        """
        æµå¼ç”Ÿæˆè¿½é—®çš„å›ç­”

        Args:
            selected_text: åŸå§‹é€‰ä¸­æ–‡æœ¬
            initial_insight: åˆå§‹æ´å¯Ÿå†…å®¹
            conversation_history: å¯¹è¯å†å²
            follow_up_question: è¿½é—®é—®é¢˜
            use_reasoning: æ˜¯å¦ä½¿ç”¨æ¨ç†æ¨¡å‹

        Yields:
            str | dict: AI ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µæˆ–åŒ…å«æ¨ç†å†…å®¹çš„å­—å…¸
        """
        # é€‰æ‹©æ¨¡å‹
        self.model_used = settings.reasoning_model if use_reasoning else settings.default_model

        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„å­¦ä¹ åŠ©æ‰‹ã€‚ç”¨æˆ·æ­£åœ¨æ·±å…¥ç†è§£æŸä¸ªæ¦‚å¿µï¼Œä½ éœ€è¦æ ¹æ®å¯¹è¯å†å²ï¼Œå›ç­”ä»–çš„è¿½é—®ã€‚"
            },
            {
                "role": "user",
                "content": f"åŸå§‹æ–‡æœ¬: {selected_text}\n\nåˆå§‹æ´å¯Ÿ: {initial_insight}"
            }
        ]

        # æ·»åŠ å¯¹è¯å†å²
        for msg in conversation_history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })

        # æ·»åŠ å½“å‰è¿½é—®
        messages.append({
            "role": "user",
            "content": follow_up_question
        })

        # è°ƒç”¨ API
        try:
            stream = await self.client.chat.completions.create(
                model=self.model_used,
                max_tokens=settings.max_tokens,
                temperature=settings.temperature,
                messages=messages,
                stream=True
            )

            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta

                    # æ”¯æŒæ¨ç†æ¨¡å¼
                    if use_reasoning:
                        result = {}
                        if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                            result['reasoning'] = delta.reasoning_content
                        if delta.content:
                            result['content'] = delta.content

                        if result:
                            yield result
                    else:
                        if delta.content:
                            yield delta.content

        except Exception as e:
            # è®°å½•LLMè°ƒç”¨é”™è¯¯
            log_llm_error(
                service_name="ai_service_follow_up_answer",
                model_name=self.model_used,
                error=e,
                request_data={
                    "selected_text_length": len(selected_text),
                    "follow_up_question": follow_up_question,
                    "use_reasoning": use_reasoning,
                    "conversation_history_length": len(conversation_history)
                }
            )
            logger.error(f"âŒ ç”Ÿæˆè¿½é—®å›ç­”å¤±è´¥: {e}")
            raise

    async def generate_simple_response(self, prompt: str) -> str:
        """
        ç”Ÿæˆç®€å•å“åº”ï¼ˆéæµå¼ï¼‰

        ç”¨äºç«èŠ±è§£é‡Šç­‰ç®€å•åœºæ™¯

        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯

        Returns:
            AI ç”Ÿæˆçš„æ–‡æœ¬å“åº”
        """
        try:
            response = await self.client.chat.completions.create(
                model=settings.default_model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            # è®°å½•LLMè°ƒç”¨é”™è¯¯
            log_llm_error(
                service_name="ai_service_simple_response",
                model_name=settings.default_model,
                error=e,
                request_data={
                    "prompt_length": len(prompt)
                }
            )
            logger.error(f"âŒ ç”Ÿæˆç®€å•å“åº”å¤±è´¥: {e}")
            raise
